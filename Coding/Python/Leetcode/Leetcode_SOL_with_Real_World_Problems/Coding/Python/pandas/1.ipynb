{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\UNB\\\\Programs\\\\scrapenovels.py\\\\Novels excel files\\\\kitabnagridotcom\\\\merge.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m file = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUNB\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mPrograms\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mscrapenovels.py\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mNovels excel files\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mkitabnagridotcom\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmerge.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'D:\\\\UNB\\\\Programs\\\\scrapenovels.py\\\\Novels excel files\\\\kitabnagridotcom\\\\merge.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = r\"D:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\kitabnagridotcom\\merge.xlsx\"\n",
    "data = pd.read_excel(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas ka display option badhao\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)  # Sabhi rows show hongi\n",
    "pd.set_option('display.max_columns', None)  # Sabhi columns bhi show hongay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())  # Ye sab column names ko list me print karega\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.Title → Dot Notation (.)\n",
    "Agar aap data.Title likh rahe hain, to iska bhi same matlab hai:\n",
    "🔹 Ye shortcut syntax hai data['Title'] ka, lekin har column ke liye kaam nahi karta.\n",
    "\n",
    "Example:\n",
    "print(data.Title)  # Same as data['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kab Avoid Karna Chahiye?\n",
    "🚨 Agar column ka naam space ya special character contain karta ho, to dot notation (data.Title) error de sakti hai:\n",
    "\n",
    "data[\"Book Title\"]  # ✅ Works\n",
    "data.Book Title  # ❌ Error (Space in column name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data['Title'] () → Brackets (())\n",
    "Agar aap data['Title'] () likh rahe hain, to iska matlab hai Title column pe kisi function ka apply hona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Title\"] = data[\"Title\"].apply(lambda x: x.upper())\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Title','Google Drive Links']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Title','Google Drive Links']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[[1,5,6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:,[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data['Title']=='Ishq ki Sargazisht Novel Complete Free Pdf By Reem Farooq'\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data[mask]  # Returns rows where \"Title\" is \"namal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data[data['Title'] == 'pdf']\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data[data['Title'].str.lower().str.contains('pdf', na=False)]\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "display(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.to_excel(\"filtered_titles.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Google Drive Links'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Google Drive Links'].value_counts().head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_titles = \" \".join(df['Title']).lower().split()  # Saare words ek list me\n",
    "word_freq = Counter(all_titles)  # Word count\n",
    "print(word_freq.most_common(20))  # Top 20 common words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df['Title'].str.contains(\"umera\", case=False, na=False)]\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())  # Ye sab column names ko list me print karega\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook me ye sab variables show karega\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aap ke kehne ke mutabiq, agar Title column mein kahin bhi \"Download\", \"pdf\", ya dono shamil hoon, to wo words hata diye jayen lekin baaki title waise ke waise rahen — to yahaan ek Python code diya gaya hai jo pandas ka use karta hai aur result ko ek naye Excel file mein save karta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Excel file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\Besturdubooks_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Original Excel file ka path\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\Besturdubooks_ok\\ok1.xlsx\"  # Replace this with your actual filename\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\Besturdubooks_ok\\ready.xlsx\"  # Naya file yahi naam se banega\n",
    "\n",
    "# Excel file read karo\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Title column mein se 'download' aur 'pdf' hatao (case insensitive)\n",
    "def clean_title(title):\n",
    "    return re.sub(r'\\b(download|pdf)\\b', '', str(title), flags=re.IGNORECASE).strip()\n",
    "\n",
    "df['Title'] = df['Title'].apply(clean_title)\n",
    "\n",
    "# Nayi file save karo\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"Cleaned Excel file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediafire link se title extract karna — jaise:\n",
    "\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "https://www.mediafire.com/file/8chlaul1wvevvsj/Jo+chale+to+jan+se+guzar+gaey-min.pdf/file\n",
    "Se title nikaalna:\n",
    "\n",
    "arduino\n",
    "Copy\n",
    "Edit\n",
    "Jo chale to jan se guzar gaey-min.pdf\n",
    "Phir Title column ke andar is title ko replace karna (agar Mediafire Links column mein koi link ho), aur phir result ko new Excel file mein save karna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Excel file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\digestkahani_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# Original Excel file\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\digestkahani_ok\\ok3.xlsx\" # Replace with your file name\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\digestkahani_ok\\ready.xlsx\"\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to extract title from Mediafire link\n",
    "def extract_title_from_link(link):\n",
    "    if isinstance(link, str) and 'mediafire.com' in link:\n",
    "        match = re.search(r'/file/.+?/([^/]+)', link)\n",
    "        if match:\n",
    "            filename = match.group(1)\n",
    "            filename = unquote(filename.replace('+', ' '))  # Decode URL and replace + with space\n",
    "            return filename\n",
    "    return None\n",
    "\n",
    "# Replace Title where Mediafire link exists\n",
    "for idx, row in df.iterrows():\n",
    "    mediafire_link = row.get(\"Mediafire Links\", \"\")\n",
    "    extracted_title = extract_title_from_link(mediafire_link)\n",
    "    if extracted_title:\n",
    "        df.at[idx, \"Title\"] = extracted_title\n",
    "\n",
    "# Save to new Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"New Excel file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Aap chahte hain ke:\n",
    "\n",
    "Title column mein agar kahin bhi .pdf likha ho, to usse remove kar diya jaye.\n",
    "\n",
    "Ye kaam Mediafire link se title extract karne ke baad bhi apply ho.\n",
    "\n",
    "Yeh raha final Python code jo sab kuch karta hai:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New Excel file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\digestkahani_ok\\ready1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# Original Excel file\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\digestkahani_ok\\ready.xlsx\" # ← replace with your file name\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\digestkahani_ok\\ready1.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to extract and clean title from Mediafire link\n",
    "def extract_title_from_link(link):\n",
    "    if isinstance(link, str) and 'mediafire.com' in link:\n",
    "        match = re.search(r'/file/.+?/([^/]+)', link)\n",
    "        if match:\n",
    "            filename = match.group(1)\n",
    "            filename = unquote(filename.replace('+', ' '))  # Convert + to space and decode\n",
    "            filename = filename.replace('.pdf', '').strip()  # Remove .pdf from title\n",
    "            return filename\n",
    "    return None\n",
    "\n",
    "# Update Title column with extracted title from Mediafire Links\n",
    "for idx, row in df.iterrows():\n",
    "    mediafire_link = row.get(\"Mediafire Links\", \"\")\n",
    "    extracted_title = extract_title_from_link(mediafire_link)\n",
    "    if extracted_title:\n",
    "        df.at[idx, \"Title\"] = extracted_title\n",
    "\n",
    "# Also remove '.pdf' from existing Title values (just in case)\n",
    "df['Title'] = df['Title'].str.replace('.pdf', '', case=False).str.strip()\n",
    "\n",
    "# Save to new Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"✅ New Excel file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title column mein jahan jahan .pdf - Google Drive likha ho, usay remove kar diya jaye.\n",
    "\n",
    "Aur result usi folder mein new Excel file ke taur par save ho jaye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\DigestLibrarydotcom_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input aur output file ka naam\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\DigestLibrarydotcom_ok\\merged_output.xlsx\"# ← yahan apni file ka naam daalein\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\DigestLibrarydotcom_ok\\ready.xlsx\"\n",
    "\n",
    "# Excel file read karo\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# 'Title' column mein '.pdf - Google Drive' ko remove karo\n",
    "df['Title'] = df['Title'].str.replace('.pdf - Google Drive', '', case=False).str.strip()\n",
    "\n",
    "# Nayi file save karo\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"✅ Cleaned file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title column mein jahan jahan \"free pdf download\" likha ho (case-insensitive), usay remove kar diya jaye.\n",
    "\n",
    "Baaki title bilkul waise ka waise rahe.\n",
    "\n",
    "Naya Excel file save ho jaye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Words removed and file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\freepdf_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Step 1: File names\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\freepdf_ok\\ok7.xlsx\"  # ← replace with your actual file name\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\freepdf_ok\\ready.xlsx\"\n",
    "\n",
    "# Step 2: Load Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Step 3: Function to clean unwanted words from title\n",
    "def clean_title(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove words: free, download, online (case-insensitive)\n",
    "        cleaned = re.sub(r'\\b(free|download|online)\\b', '', text, flags=re.IGNORECASE)\n",
    "        return re.sub(r'\\s+', ' ', cleaned).strip()  # Clean extra spaces\n",
    "    return text\n",
    "\n",
    "# Step 4: Apply function to 'Title' column\n",
    "df['Title'] = df['Title'].apply(clean_title)\n",
    "\n",
    "# Step 5: Save cleaned DataFrame\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"✅ Words removed and file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title column mein jahan jahan bhi dot (.) ho, usay remove kar diya jaye.\n",
    "\n",
    "Baaki text bilkul same rahe.\n",
    "\n",
    "Aur result ek naye Excel file mein save ho jaye.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dots removed and file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\novelsjahan_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input and Output file\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\novelsjahan_ok\\ok14.xlsx\"  # ← apni actual file ka naam yahan likhein\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\novelsjahan_ok\\ready.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Remove all dots (.) from 'Title' column\n",
    "df['Title'] = df['Title'].str.replace('.', '', regex=False)\n",
    "\n",
    "# Save the cleaned DataFrame to a new Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"✅ Dots removed and file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Phrase removed successfully. File saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\urdunovelsghar_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File names\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\urdunovelsghar_ok\\ok21.xlsx\" # ← apni file ka naam yahan likhein\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\urdunovelsghar_ok\\ready.xlsx\"\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define cleaner function\n",
    "def clean_phrase(title):\n",
    "    if isinstance(title, str):\n",
    "        # Remove the full phrase (case-insensitive)\n",
    "        cleaned = re.sub(r'download pdf complete free of', '', title, flags=re.IGNORECASE)\n",
    "        return re.sub(r'\\s+', ' ', cleaned).strip()  # Remove extra spaces\n",
    "    return title\n",
    "\n",
    "# Apply to 'Title' column\n",
    "df['Title'] = df['Title'].apply(clean_phrase)\n",
    "\n",
    "# Save new Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"✅ Phrase removed successfully. File saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ '- Urdu Readings' removed successfully. File saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\urdureadings_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File names\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\urdureadings_ok\\ok23.xlsx\" # ← apni file ka naam yahan likhein\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\urdureadings_ok\\ready.xlsx\"\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define function to remove \"- Urdu Readings\" (with optional spaces before/after dash)\n",
    "def clean_urdu_readings(title):\n",
    "    if isinstance(title, str):\n",
    "        cleaned = re.sub(r'\\s*-\\s*Urdu Readings', '', title, flags=re.IGNORECASE)\n",
    "        return cleaned.strip()\n",
    "    return title\n",
    "\n",
    "# Apply cleaning\n",
    "df['Title'] = df['Title'].apply(clean_urdu_readings)\n",
    "\n",
    "# Save cleaned file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"✅ '- Urdu Readings' removed successfully. File saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ '– ZNZ' removed successfully. File saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\zubinovelszone\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File names\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\zubinovelszone\\merged_output.xlsx\" # ← Replace this with your actual file\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\zubinovelszone\\ready.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to remove '– ZNZ' (with long dash and optional spaces)\n",
    "def clean_znz(text):\n",
    "    if isinstance(text, str):\n",
    "        cleaned = re.sub(r'\\s*–\\s*ZNZ', '', text, flags=re.IGNORECASE)\n",
    "        return cleaned.strip()\n",
    "    return text\n",
    "\n",
    "# Apply the function to Title column\n",
    "df['Title'] = df['Title'].apply(clean_znz)\n",
    "\n",
    "# Save the cleaned Excel\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"✅ '– ZNZ' removed successfully. File saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Excel file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\okkkkkkkkkkk.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# Original Excel file\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\merged_output.xlsx\"  # Replace with your file name\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\okkkkkkkkkkk.xlsx\"\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to extract title from Mediafire link\n",
    "def extract_title_from_link(link):\n",
    "    if isinstance(link, str) and 'mediafire.com' in link:\n",
    "        match = re.search(r'/file/.+?/([^/]+)', link)\n",
    "        if match:\n",
    "            filename = match.group(1)\n",
    "            filename = unquote(filename.replace('+', ' '))  # Decode URL and replace + with space\n",
    "            return filename\n",
    "    return None\n",
    "\n",
    "# Replace Title where Mediafire link exists\n",
    "for idx, row in df.iterrows():\n",
    "    mediafire_link = row.get(\"Mediafire Links\", \"\")\n",
    "    extracted_title = extract_title_from_link(mediafire_link)\n",
    "    if extracted_title:\n",
    "        df.at[idx, \"Title\"] = extracted_title\n",
    "\n",
    "# Save to new Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"New Excel file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input aur output file ka naam\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\okkkkkkkkkkk.xlsx\"  # ← yahan apni file ka naam daalein\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\okok.xlsx\"\n",
    "\n",
    "# Excel file read karo\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# 'Title' column mein '.pdf - Google Drive' ko remove karo\n",
    "df['Title'] = df['Title'].str.replace('.pdf', '', case=False).str.strip()\n",
    "\n",
    "# Nayi file save karo\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"✅ Cleaned file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ '.pdf' and website names removed. File saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\olll.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\okok.xlsx\" # ← Replace with your file name\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\olll.xlsx\"\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to clean '.pdf' and website domains\n",
    "def clean_title(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove .pdf\n",
    "        text = re.sub(r'\\.pdf', '', text, flags=re.IGNORECASE)\n",
    "        # Remove website names (ending with .com, .org, .net etc.)\n",
    "        text = re.sub(r'\\b[\\w.-]+\\.(com|org|net|info|pk|in|us)\\b', '', text, flags=re.IGNORECASE)\n",
    "        # Remove multiple spaces\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['Title'] = df['Title'].apply(clean_title)\n",
    "\n",
    "# Save cleaned file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"✅ '.pdf' and website names removed. File saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All brackets and their content removed. File saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\ollllllll.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File names\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\olll.xlsx\" # ← Replace with your file name\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\ollllllll.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to remove all types of brackets and their content\n",
    "def remove_brackets(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove (), [], {} with content\n",
    "        text = re.sub(r'\\(.*?\\)|\\[.*?\\]|\\{.*?\\}', '', text)\n",
    "        # Clean up extra spaces\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply to Title column\n",
    "df['Title'] = df['Title'].apply(remove_brackets)\n",
    "\n",
    "# Save cleaned file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"✅ All brackets and their content removed. File saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Agar Title column mein \"DOWNLOAD FREE URDU BOOKS AND NOVELS\" likha ho, to:\n",
    "\n",
    "Usi row ka Mediafire link uthaya jaye\n",
    "\n",
    "Us link se PDF file ka naam extract kiya jaye\n",
    "\n",
    "Us naam mein jitne bhi + symbols hon, unhein space se replace kiya jaye\n",
    "\n",
    "Aur phir \"DOWNLOAD FREE URDU BOOKS AND NOVELS\" ko us cleaned title se replace kar diya jaye\n",
    "\n",
    "Final result ek naye Excel file mein save ho jaye ✅\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned file saved as: E:\\UNB\\oknovels\\mooot1111.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# Input and Output files\n",
    "input_file = r\"E:\\UNB\\oknovels\\mooot.xlsx\" # 👈 Replace this with your Excel file name\n",
    "output_file = r\"E:\\UNB\\oknovels\\mooot1111.xlsx\"\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to extract clean title from mediafire link\n",
    "def clean_title_from_link(link):\n",
    "    if isinstance(link, str) and \"mediafire.com\" in link:\n",
    "        try:\n",
    "            file_part = link.strip().split(\"/\")[-1]\n",
    "            file_part = unquote(file_part)  # decode URL\n",
    "            file_part = file_part.replace(\"+\", \" \")\n",
    "            file_part = re.sub(r'\\.pdf$', '', file_part, flags=re.IGNORECASE)\n",
    "            file_part = re.sub(r'\\burdunovelist\\.blogspot\\.com\\b', '', file_part, flags=re.IGNORECASE)\n",
    "            return file_part.strip()\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Replace titles with mediafire titles if title has \"DOWNLOAD FREE URDU BOOKS AND NOVELS\"\n",
    "for i, row in df.iterrows():\n",
    "    title = str(row['Title'])\n",
    "    mediafire_link = row.get(\"Mediafire Links\", \"\")\n",
    "    if \"Urdu Books\" in title.upper():\n",
    "        extracted_title = clean_title_from_link(mediafire_link)\n",
    "        if extracted_title:\n",
    "            df.at[i, 'Title'] = extracted_title\n",
    "\n",
    "# Save the cleaned Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "print(\"✅ Cleaned file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the Excel file\n",
    "input_file = r\"C:\\Users\\PCS\\Downloads\\urdunovelbanks\\Blogger_Novels.xlsx\" # Replace with your actual file path\n",
    "try:\n",
    "    df = pd.read_excel(input_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {input_file} was not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "# Step 1.5: Print column names to confirm\n",
    "print(\"Column names in the Excel file:\", df.columns.tolist())\n",
    "\n",
    "# Step 2: Filter out rows where both columns have \"No Google Drive Link\" and \"No Mediafire Link\"\n",
    "try:\n",
    "    filtered_df = df[~((df['Google Drive Links'] == 'No Google Drive Link') & (df['Mediafire Links'] == 'No Mediafire Link'))]\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e}. One of the columns was not found in the Excel file.\")\n",
    "    print(\"Please check the column names printed above and update the code with the correct column names.\")\n",
    "    exit()\n",
    "\n",
    "# Step 3: Save the filtered data to a new Excel file\n",
    "output_file = r\"C:\\Users\\PCS\\Downloads\\urdunovelbanks\\ok.xlsx\" # Replace with your desired output file path\n",
    "filtered_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Filtered data has been saved to {output_file}\")\n",
    "print(f\"Original number of rows: {len(df)}\")\n",
    "print(f\"Number of rows after filtering: {len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Excel file 'mooot666.xlsx' has been created with updated links!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the Excel file\n",
    "df = pd.read_excel(r\"E:\\UNB\\oknovels\\mooot.xlsx\")\n",
    "\n",
    "# Step 2: Function to determine the link to use for the \"Links\" column\n",
    "def get_available_link(row):\n",
    "    google_link = row['Google Drive Links']\n",
    "    mediafire_link = row['Mediafire Links']\n",
    "    \n",
    "    # Check if Google Drive link is valid (not \"No Google Drive Link\" or NaN)\n",
    "    if pd.notna(google_link) and \"No Google Drive Link\" not in str(google_link):\n",
    "        return google_link  # Prioritize Google Drive link if available\n",
    "    # If Google Drive link is not available, use Mediafire link if it exists\n",
    "    elif pd.notna(mediafire_link) and \"No Google Drive Link\" not in str(mediafire_link):\n",
    "        return mediafire_link  # Use Mediafire link if Google Drive is not available\n",
    "    else:\n",
    "        return \"No Link Available\"\n",
    "\n",
    "# Step 3: Apply the function to populate the \"Links\" column\n",
    "df['Links'] = df.apply(get_available_link, axis=1)\n",
    "\n",
    "# Step 4: Save the updated dataframe to a new Excel file\n",
    "df.to_excel(r\"E:\\UNB\\oknovels\\mooot666.xlsx\", index=False)\n",
    "\n",
    "print(\"New Excel file 'mooot666.xlsx' has been created with updated links!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Excel file 'mooot666.xlsx' has been created with updated links!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the Excel file\n",
    "df = pd.read_excel(r\"E:\\UNB\\oknovels\\mooot.xlsx\")\n",
    "\n",
    "# Step 2: Function to determine the link to use for the \"Links\" column\n",
    "def get_available_link(row):\n",
    "    google_link = row['Google Drive Links']\n",
    "    mediafire_link = row['Mediafire Links']\n",
    "    \n",
    "    # Check if Google Drive link is valid (not \"No Google Drive Link\" or NaN)\n",
    "    if pd.notna(google_link) and \"No Google Drive Link\" not in str(google_link):\n",
    "        return google_link  # Prioritize Google Drive link if available\n",
    "    # If Google Drive link is not available, use Mediafire link if it exists\n",
    "    elif pd.notna(mediafire_link) and \"No Google Drive Link\" not in str(mediafire_link):\n",
    "        return mediafire_link  # Use Mediafire link if Google Drive is not available\n",
    "    else:\n",
    "        return \"No Link Available\"\n",
    "\n",
    "# Step 3: Apply the function to populate the \"Links\" column\n",
    "df['Links'] = df.apply(get_available_link, axis=1)\n",
    "\n",
    "# Step 4: Function to keep only one link in the \"Links\" column\n",
    "def keep_one_link(links):\n",
    "    # If the links value is not a string or is \"No Link Available\", return as is\n",
    "    if not isinstance(links, str) or links == \"No Link Available\":\n",
    "        return links\n",
    "    \n",
    "    # Split the links by comma\n",
    "    link_list = [link.strip() for link in links.split(\",\")]\n",
    "    \n",
    "    # Look for the first Google Drive link\n",
    "    for link in link_list:\n",
    "        if \"drive.google.com\" in link:\n",
    "            return link\n",
    "    \n",
    "    # If no Google Drive link is found, return the first link\n",
    "    return link_list[0]\n",
    "\n",
    "# Step 5: Apply the function to keep only one link in the \"Links\" column\n",
    "df['Links'] = df['Links'].apply(keep_one_link)\n",
    "\n",
    "# Step 6: Save the updated dataframe to a new Excel file\n",
    "df.to_excel(r\"E:\\UNB\\oknovels\\mooot666.xlsx\", index=False)\n",
    "\n",
    "print(\"New Excel file 'mooot666.xlsx' has been created with updated links!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links column successfully updated and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Excel file ka path\n",
    "file_path = r\"E:\\UNB\\oknovels\\mooot.xlsx\"  # <-- yahan apni Excel file ka actual path den\n",
    "\n",
    "# Excel file load karein\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Links column banayein ya update karein\n",
    "def get_preferred_link(row):\n",
    "    gdrive = row['Google Drive Links']\n",
    "    mediafire = row['Mediafire Links']\n",
    "    \n",
    "    if pd.notna(gdrive) and 'No Google Drive Link' not in str(gdrive):\n",
    "        return gdrive\n",
    "    elif pd.notna(mediafire) and 'No Mediafire Link' not in str(mediafire):\n",
    "        return mediafire\n",
    "    else:\n",
    "        return 'No Link Available'\n",
    "\n",
    "# New column add karein\n",
    "df['Links'] = df.apply(get_preferred_link, axis=1)\n",
    "\n",
    "# Updated Excel file save karein\n",
    "df.to_excel(r\"E:\\UNB\\oknovels\\mooot1.xlsx\", index=False)\n",
    "\n",
    "print(\"Links column successfully updated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved as: E:\\UNB\\oknovels\\moootok.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Excel file ko read karna\n",
    "def update_links_column(excel_file_path, output_file_path):\n",
    "    # Excel file ko pandas DataFrame mein load karna\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "    \n",
    "    # Links column ko initially empty string se fill karna\n",
    "    df['Links'] = ''\n",
    "    \n",
    "    # Har row ke liye Links column ko update karna\n",
    "    for index, row in df.iterrows():\n",
    "        google_drive_link = row['Google Drive Links']\n",
    "        mediafire_link = row['Mediafire Links']\n",
    "        \n",
    "        # Condition 1: Agar dono links available hain to Google Drive link ko prefer karna\n",
    "        if google_drive_link != 'No Google Drive Link' and mediafire_link != 'No Mediafire Link':\n",
    "            df.at[index, 'Links'] = google_drive_link\n",
    "        \n",
    "        # Condition 2: Agar sirf Google Drive link available hai\n",
    "        elif google_drive_link != 'No Google Drive Link' and mediafire_link == 'No Mediafire Link':\n",
    "            df.at[index, 'Links'] = google_drive_link\n",
    "        \n",
    "        # Condition 3: Agar sirf Mediafire link available hai\n",
    "        elif google_drive_link == 'No Google Drive Link' and mediafire_link != 'No Mediafire Link':\n",
    "            df.at[index, 'Links'] = mediafire_link\n",
    "    \n",
    "    # Updated DataFrame ko new Excel file mein save karna\n",
    "    df.to_excel(output_file_path, index=False)\n",
    "    print(f\"Updated file saved as: {output_file_path}\")\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\oknovels\\mooot.xlsx\"  # Apni input Excel file ka path\n",
    "output_file = r\"E:\\UNB\\oknovels\\moootok.xlsx\" # Output Excel file ka path\n",
    "\n",
    "# Function call\n",
    "update_links_column(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total titles processed: 78293\n",
      "Titles with missing links: 6884\n",
      "Titles with no links available:\n",
      "- nan\n",
      "- Tere bakhat ki roshani\n",
      "- Phir hua yun\n",
      "- Susral gaidha phool\n",
      "- Jana tere leye\n",
      "- Fasle by Razia But\n",
      "- Hum aur bulbulen complete\n",
      "- sare diye jalte rahe complete\n",
      "- Sagar darya badal boond by Razia Jamil\n",
      "- Ibn e Maryam Novel by Aalia Hira - Zemtime.com\n",
      "... and 6874 more titles\n",
      "Updated file saved as: E:\\UNB\\oknovels\\moootnew.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def update_links_column(excel_file_path, output_file_path):\n",
    "    # Excel file ko pandas DataFrame mein load karna\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "    \n",
    "    # Data ko clean karna: Titles aur links ke columns se extra spaces remove karna\n",
    "    df['Titles'] = df['Titles'].str.strip()\n",
    "    df['Google Drive Links'] = df['Google Drive Links'].str.strip()\n",
    "    df['Mediafire Links'] = df['Mediafire Links'].str.strip()\n",
    "    \n",
    "    # Links column ko initially empty string se fill karna\n",
    "    df['Links'] = ''\n",
    "    \n",
    "    # Missing links ko track karne ke liye counter aur list\n",
    "    missing_links_count = 0\n",
    "    missing_links_titles = []\n",
    "    \n",
    "    # Har row ke liye Links column ko update karna\n",
    "    for index, row in df.iterrows():\n",
    "        google_drive_link = row['Google Drive Links']\n",
    "        mediafire_link = row['Mediafire Links']\n",
    "        \n",
    "        # Check karna ke link columns mein valid data hai ya nahi\n",
    "        has_google_drive = (google_drive_link != 'No Google Drive Link' and \n",
    "                          pd.notna(google_drive_link) and \n",
    "                          google_drive_link != '')\n",
    "        has_mediafire = (mediafire_link != 'No Mediafire Link' and \n",
    "                        pd.notna(mediafire_link) and \n",
    "                        mediafire_link != '')\n",
    "        \n",
    "        # Condition 1: Agar dono links available hain to Google Drive link ko prefer karna\n",
    "        if has_google_drive and has_mediafire:\n",
    "            df.at[index, 'Links'] = google_drive_link\n",
    "        \n",
    "        # Condition 2: Agar sirf Google Drive link available hai\n",
    "        elif has_google_drive and not has_mediafire:\n",
    "            df.at[index, 'Links'] = google_drive_link\n",
    "        \n",
    "        # Condition 3: Agar sirf Mediafire link available hai\n",
    "        elif not has_google_drive and has_mediafire:\n",
    "            df.at[index, 'Links'] = mediafire_link\n",
    "        \n",
    "        # Condition 4: Agar dono links nahi hain to warning dena\n",
    "        else:\n",
    "            missing_links_count += 1\n",
    "            missing_links_titles.append(row['Titles'])\n",
    "            df.at[index, 'Links'] = 'No Link Available'\n",
    "    \n",
    "    # Updated DataFrame ko new Excel file mein save karna\n",
    "    df.to_excel(output_file_path, index=False)\n",
    "    \n",
    "    # Logging: Kitne titles ke links nahi mile aur unke names\n",
    "    print(f\"Total titles processed: {len(df)}\")\n",
    "    print(f\"Titles with missing links: {missing_links_count}\")\n",
    "    if missing_links_titles:\n",
    "        print(\"Titles with no links available:\")\n",
    "        for title in missing_links_titles[:10]:  # Sirf pehle 10 titles print karenge, taake output zyada lambi na ho\n",
    "            print(f\"- {title}\")\n",
    "        if len(missing_links_titles) > 10:\n",
    "            print(f\"... and {len(missing_links_titles) - 10} more titles\")\n",
    "\n",
    "    print(f\"Updated file saved as: {output_file_path}\")\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\oknovels\\mooot.xlsx\"   # Apni input Excel file ka path\n",
    "output_file =r\"E:\\UNB\\oknovels\\moootnew.xlsx\"  # Output Excel file ka path\n",
    "\n",
    "# Function call\n",
    "update_links_column(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file ke column names:\n",
      "['Titles', 'Google Drive Links', 'Mediafire Links', 'Links']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input file path\n",
    "input_file = r\"E:\\UNB\\oknovels\\mooot.xlsx\"\n",
    "\n",
    "# Excel file ko read karna\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Column names print karna\n",
    "print(\"Excel file ke column names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file ke column names (exact):\n",
      "['Titles', 'Google Drive Links', 'Mediafire Links', 'Links']\n",
      "Total titles processed: 78293\n",
      "Titles with missing links: 6884\n",
      "Titles with no links available:\n",
      "- nan\n",
      "- Tere bakhat ki roshani\n",
      "- Phir hua yun\n",
      "- Susral gaidha phool\n",
      "- Jana tere leye\n",
      "- Fasle by Razia But\n",
      "- Hum aur bulbulen complete\n",
      "- sare diye jalte rahe complete\n",
      "- Sagar darya badal boond by Razia Jamil\n",
      "- Ibn e Maryam Novel by Aalia Hira - Zemtime.com\n",
      "... and 6874 more titles\n",
      "Updated file saved as: E:\\UNB\\oknovels\\moootnew.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def update_links_column(excel_file_path, output_file_path):\n",
    "    try:\n",
    "        # Excel file ko pandas DataFrame mein load karna\n",
    "        df = pd.read_excel(excel_file_path)\n",
    "        \n",
    "        # Column names ko print karna taake confirm kar sakein\n",
    "        print(\"Excel file ke column names (exact):\")\n",
    "        print(df.columns.tolist())\n",
    "        \n",
    "        # Column names ko clean karna (extra spaces remove karna)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Check karna ke required columns exist karte hain\n",
    "        required_columns = ['Titles', 'Google Drive Links', 'Mediafire Links']\n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                raise KeyError(f\"Column '{col}' not found in Excel file. Available columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Links column ko add karna agar already nahi hai\n",
    "        if 'Links' not in df.columns:\n",
    "            df['Links'] = ''\n",
    "        \n",
    "        # Data ko clean karna: Titles aur links ke columns se extra spaces remove karna\n",
    "        df['Titles'] = df['Titles'].str.strip()\n",
    "        df['Google Drive Links'] = df['Google Drive Links'].str.strip()\n",
    "        df['Mediafire Links'] = df['Mediafire Links'].str.strip()\n",
    "        \n",
    "        # Missing links ko track karne ke liye counter aur list\n",
    "        missing_links_count = 0\n",
    "        missing_links_titles = []\n",
    "        \n",
    "        # Har row ke liye Links column ko update karna\n",
    "        for index, row in df.iterrows():\n",
    "            google_drive_link = row['Google Drive Links']\n",
    "            mediafire_link = row['Mediafire Links']\n",
    "            \n",
    "            # Check karna ke link columns mein valid data hai ya nahi\n",
    "            has_google_drive = (google_drive_link != 'No Google Drive Link' and \n",
    "                              pd.notna(google_drive_link) and \n",
    "                              google_drive_link != '')\n",
    "            has_mediafire = (mediafire_link != 'No Mediafire Link' and \n",
    "                            pd.notna(mediafire_link) and \n",
    "                            mediafire_link != '')\n",
    "            \n",
    "            # Condition 1: Agar dono links available hain to Google Drive link ko prefer karna\n",
    "            if has_google_drive and has_mediafire:\n",
    "                df.at[index, 'Links'] = google_drive_link\n",
    "            \n",
    "            # Condition 2: Agar sirf Google Drive link available hai\n",
    "            elif has_google_drive and not has_mediafire:\n",
    "                df.at[index, 'Links'] = google_drive_link\n",
    "            \n",
    "            # Condition 3: Agar sirf Mediafire link available hai\n",
    "            elif not has_google_drive and has_mediafire:\n",
    "                df.at[index, 'Links'] = mediafire_link\n",
    "            \n",
    "            # Condition 4: Agar dono links nahi hain to warning dena\n",
    "            else:\n",
    "                missing_links_count += 1\n",
    "                missing_links_titles.append(row['Titles'])\n",
    "                df.at[index, 'Links'] = 'No Link Available'\n",
    "        \n",
    "        # Updated DataFrame ko new Excel file mein save karna\n",
    "        df.to_excel(output_file_path, index=False)\n",
    "        \n",
    "        # Logging: Kitne titles ke links nahi mile aur unke names\n",
    "        print(f\"Total titles processed: {len(df)}\")\n",
    "        print(f\"Titles with missing links: {missing_links_count}\")\n",
    "        if missing_links_titles:\n",
    "            print(\"Titles with no links available:\")\n",
    "            for title in missing_links_titles[:10]:  # Sirf pehle 10 titles print karenge\n",
    "                print(f\"- {title}\")\n",
    "            if len(missing_links_titles) > 10:\n",
    "                print(f\"... and {len(missing_links_titles) - 10} more titles\")\n",
    "\n",
    "        print(f\"Updated file saved as: {output_file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\oknovels\\mooot.xlsx\"  # Input Excel file ka path\n",
    "output_file = r\"E:\\UNB\\oknovels\\moootnew.xlsx\"  # Output Excel file ka path\n",
    "\n",
    "# Function call\n",
    "update_links_column(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file ke column names (exact):\n",
      "['Titles', 'Google Drive Links', 'Mediafire Links', 'Links']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PCS\\AppData\\Local\\Temp\\ipykernel_3860\\1538312945.py:53: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'https://drive.google.com/file/d/1d34nWpBlEZmRsAMSKcqI8EXbKkwFyu26/view?usp=sharing, https://drive.google.com/file/d/1d34nWpBlEZmRsAMSKcqI8EXbKkwFyu26/view?usp=sharing' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[index, 'Links'] = google_drive_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total titles processed before deletion: 71410\n",
      "Titles with missing links: 1\n",
      "Titles with no links available (will be deleted):\n",
      "- nan\n",
      "Total titles after deletion: 71409\n",
      "Updated file saved as: E:\\UNB\\oknovels\\moootnew.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def update_links_column(excel_file_path, output_file_path):\n",
    "    try:\n",
    "        # Excel file ko pandas DataFrame mein load karna\n",
    "        df = pd.read_excel(excel_file_path)\n",
    "        \n",
    "        # Column names ko print karna taake confirm kar sakein\n",
    "        print(\"Excel file ke column names (exact):\")\n",
    "        print(df.columns.tolist())\n",
    "        \n",
    "        # Column names ko clean karna (extra spaces remove karna)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Check karna ke required columns exist karte hain\n",
    "        required_columns = ['Titles', 'Google Drive Links', 'Mediafire Links']\n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                raise KeyError(f\"Column '{col}' not found in Excel file. Available columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Links column ko add karna agar already nahi hai\n",
    "        if 'Links' not in df.columns:\n",
    "            df['Links'] = ''\n",
    "        \n",
    "        # Data ko clean karna: Titles aur links ke columns se extra spaces remove karna\n",
    "        df['Titles'] = df['Titles'].str.strip()\n",
    "        df['Google Drive Links'] = df['Google Drive Links'].str.strip()\n",
    "        df['Mediafire Links'] = df['Mediafire Links'].str.strip()\n",
    "        \n",
    "        # Missing links ko track karne ke liye counter aur list\n",
    "        missing_links_count = 0\n",
    "        missing_links_titles = []\n",
    "        \n",
    "        # Har row ke liye Links column ko update karna\n",
    "        for index, row in df.iterrows():\n",
    "            google_drive_link = row['Google Drive Links']\n",
    "            mediafire_link = row['Mediafire Links']\n",
    "            \n",
    "            # Check karna ke link columns mein valid data hai ya nahi\n",
    "            has_google_drive = (google_drive_link != 'No Google Drive Link' and \n",
    "                              pd.notna(google_drive_link) and \n",
    "                              google_drive_link != '')\n",
    "            has_mediafire = (mediafire_link != 'No Mediafire Link' and \n",
    "                            pd.notna(mediafire_link) and \n",
    "                            mediafire_link != '')\n",
    "            \n",
    "            # Condition 1: Agar dono links available hain to Google Drive link ko prefer karna\n",
    "            if has_google_drive and has_mediafire:\n",
    "                df.at[index, 'Links'] = google_drive_link\n",
    "            \n",
    "            # Condition 2: Agar sirf Google Drive link available hai\n",
    "            elif has_google_drive and not has_mediafire:\n",
    "                df.at[index, 'Links'] = google_drive_link\n",
    "            \n",
    "            # Condition 3: Agar sirf Mediafire link available hai\n",
    "            elif not has_google_drive and has_mediafire:\n",
    "                df.at[index, 'Links'] = mediafire_link\n",
    "            \n",
    "            # Condition 4: Agar dono links nahi hain to warning dena\n",
    "            else:\n",
    "                missing_links_count += 1\n",
    "                missing_links_titles.append(row['Titles'])\n",
    "                df.at[index, 'Links'] = 'No Link Available'\n",
    "        \n",
    "        # Logging: Kitne titles ke links nahi mile aur unke names\n",
    "        print(f\"Total titles processed before deletion: {len(df)}\")\n",
    "        print(f\"Titles with missing links: {missing_links_count}\")\n",
    "        if missing_links_titles:\n",
    "            print(\"Titles with no links available (will be deleted):\")\n",
    "            for title in missing_links_titles[:10]:  # Sirf pehle 10 titles print karenge\n",
    "                print(f\"- {title}\")\n",
    "            if len(missing_links_titles) > 10:\n",
    "                print(f\"... and {len(missing_links_titles) - 10} more titles\")\n",
    "        \n",
    "        # \"No Link Available\" wali rows ko delete karna\n",
    "        df = df[df['Links'] != 'No Link Available']\n",
    "        \n",
    "        # Updated DataFrame ko new Excel file mein save karna\n",
    "        df.to_excel(output_file_path, index=False)\n",
    "        \n",
    "        # Final logging\n",
    "        print(f\"Total titles after deletion: {len(df)}\")\n",
    "        print(f\"Updated file saved as: {output_file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\oknovels\\mooot.xlsx\"  # Input Excel file ka path\n",
    "output_file = r\"E:\\UNB\\oknovels\\moootnew.xlsx\"  # Output Excel file ka path\n",
    "\n",
    "# Function call\n",
    "update_links_column(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processed successfully! Check the output at: E:\\UNB\\oknovels\\okkkkkk - Copyokkk.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file_path = r\"E:\\UNB\\oknovels\\okkkkkk - Copy.xlsx\" # Replace with your input file path\n",
    "output_file_path = r\"E:\\UNB\\oknovels\\okkkkkk - Copyokkk.xlsx\" # Replace with your desired output file path\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(input_file_path)\n",
    "\n",
    "# Function to keep only the first link if there are multiple links\n",
    "def keep_first_link(links):\n",
    "    if isinstance(links, str):  # Check if the value is a string\n",
    "        link_list = links.split(\",\")  # Split by comma\n",
    "        if len(link_list) > 1:  # If more than one link\n",
    "            return link_list[0].strip()  # Keep the first link and remove extra spaces\n",
    "    return links  # Return as is if there's only one link or it's not a string\n",
    "\n",
    "# Apply the function to the \"Links\" column\n",
    "df[\"Links\"] = df[\"Links\"].apply(keep_first_link)\n",
    "\n",
    "# Save the modified dataframe to a new Excel file\n",
    "df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"File processed successfully! Check the output at: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Har title ko check karo\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m title \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m'\u001b[39m\u001b[33mTitles\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# \"by\" ke baad ka text extract karna (case insensitive)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     match = \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mbby\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43ms+(.*)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIGNORECASE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m match:\n\u001b[32m     18\u001b[39m         authors.append(match.group(\u001b[32m1\u001b[39m).strip())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\re\\__init__.py:177\u001b[39m, in \u001b[36msearch\u001b[39m\u001b[34m(pattern, string, flags)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch\u001b[39m(pattern, string, flags=\u001b[32m0\u001b[39m):\n\u001b[32m    175\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Excel file ka path den\n",
    "input_file = r\"E:\\UNB\\oknovels\\okkkkkk - Copyokkk.xlsx\"\n",
    "output_file = r\"E:\\UNB\\oknovels\\authors.xlsx\"\n",
    "# Excel file read karo\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Authors ko store karne ke liye list\n",
    "authors = []\n",
    "\n",
    "# Har title ko check karo\n",
    "for title in df['Titles']:\n",
    "    # \"by\" ke baad ka text extract karna (case insensitive)\n",
    "    match = re.search(r'\\bby\\b\\s+(.*)', title, re.IGNORECASE)\n",
    "    if match:\n",
    "        authors.append(match.group(1).strip())\n",
    "\n",
    "# Authors list ko DataFrame mein convert karo\n",
    "authors_df = pd.DataFrame({'Author': authors})\n",
    "\n",
    "# New Excel file save karo\n",
    "authors_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Total {len(authors)} authors extracted and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 53755 authors to E:\\UNB\\oknovels\\authors.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\oknovels\\okkkkkk - Copyokkk.xlsx\"\n",
    "output_file = r\"E:\\UNB\\oknovels\\authors.xlsx\"\n",
    "\n",
    "# Read Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Authors list\n",
    "authors = []\n",
    "\n",
    "# Check each title\n",
    "for title in df['Titles']:\n",
    "    if isinstance(title, str):  # Only apply regex if title is a string\n",
    "        match = re.search(r'\\bby\\b\\s+(.*)', title, re.IGNORECASE)\n",
    "        if match:\n",
    "            authors.append(match.group(1).strip())\n",
    "\n",
    "# Save authors to Excel\n",
    "authors_df = pd.DataFrame({'Author': authors})\n",
    "authors_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Extracted {len(authors)} authors to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19891 unique authors with titles saved to C:\\Users\\PCS\\Downloads\\authors.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r\"C:\\Users\\PCS\\Downloads\\smarturdunovelbank.xlsx\"\n",
    "output_file = r\"C:\\Users\\PCS\\Downloads\\authors.xlsx\"\n",
    "\n",
    "# Read Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# List to hold (Title, Author) pairs\n",
    "data = []\n",
    "\n",
    "# Go through each title\n",
    "for title in df['Titles']:\n",
    "    if isinstance(title, str):\n",
    "        match = re.search(r'\\bby\\b\\s+(.*)', title, re.IGNORECASE)\n",
    "        if match:\n",
    "            author = match.group(1).strip()\n",
    "            data.append((title, author))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_authors = pd.DataFrame(data, columns=['Title', 'Author'])\n",
    "\n",
    "# Remove duplicate authors (keeping first occurrence)\n",
    "df_unique = df_authors.drop_duplicates(subset='Author')\n",
    "\n",
    "# Save to Excel\n",
    "df_unique.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"{len(df_unique)} unique authors with titles saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19891 unique authors sorted and saved to C:\\Users\\PCS\\Downloads\\authorssorted.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r\"C:\\Users\\PCS\\Downloads\\authors.xlsx\"\n",
    "output_file = r\"C:\\Users\\PCS\\Downloads\\authorssorted.xlsx\"\n",
    "\n",
    "# Read Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# List to hold (Title, Author) pairs\n",
    "data = []\n",
    "\n",
    "# Go through each title\n",
    "for title in df['Titles']:\n",
    "    if isinstance(title, str):\n",
    "        match = re.search(r'\\bby\\b\\s+(.*)', title, re.IGNORECASE)\n",
    "        if match:\n",
    "            author = match.group(1).strip()\n",
    "            data.append((title, author))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_authors = pd.DataFrame(data, columns=['Title', 'Author'])\n",
    "\n",
    "# Remove duplicate authors and sort alphabetically\n",
    "df_unique = df_authors.drop_duplicates(subset='Author')\n",
    "df_unique = df_unique.sort_values(by='Author')  # <-- Sorting here\n",
    "\n",
    "# Save to Excel\n",
    "df_unique.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"{len(df_unique)} unique authors sorted and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Excel file path\n",
    "file_path = r\"C:\\Users\\PCS\\Downloads\\smarturdunovelbank.xlsx\"  # Replace with your path\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Assume the column with links is named 'Link'\n",
    "mediafire_links = df[df['Link'].str.contains('mediafire.com', na=False)].copy()\n",
    "\n",
    "# Function to check link status\n",
    "def check_mediafire_link(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            if \"file could not be found\" in response.text.lower() or \"this file is temporarily unavailable\" in response.text.lower():\n",
    "                return 'Dead'\n",
    "            return 'Alive'\n",
    "        else:\n",
    "            return 'Dead'\n",
    "    except:\n",
    "        return 'Dead'\n",
    "\n",
    "# Apply function to links\n",
    "mediafire_links['Status'] = mediafire_links['Link'].apply(check_mediafire_link)\n",
    "\n",
    "# Save result to new Excel file\n",
    "mediafire_links.to_excel('checked_mediafire_links.xlsx', index=False)\n",
    "print(\"Checking complete. Results saved to 'checked_mediafire_links.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 224/28575 [00:18<39:57, 11.82it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers=\u001b[32m5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(tqdm(executor.map(check_mediafire_link, links), total=\u001b[38;5;28mlen\u001b[39m(links)))\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m mediafire_links[\u001b[33m'\u001b[39m\u001b[33mStatus\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mcheck_links_concurrently\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmediafire_links\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLink\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Save result to new Excel file\u001b[39;00m\n\u001b[32m     49\u001b[39m output_file = \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mPCS\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDownloads\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mchecked_mediafire_links.xlsx\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mcheck_links_concurrently\u001b[39m\u001b[34m(links)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_links_concurrently\u001b[39m(links):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers=\u001b[32m5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_mediafire_link\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Excel file path\n",
    "file_path = r\"C:\\Users\\PCS\\Downloads\\smarturdunovelbank.xlsx\"\n",
    "\n",
    "# Load Excel file\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "# Check if 'Link' column exists\n",
    "if 'Link' not in df.columns:\n",
    "    raise ValueError(\"Column 'Link' not found in the Excel file.\")\n",
    "\n",
    "# Filter MediaFire links\n",
    "mediafire_links = df[df['Link'].astype(str).str.contains('mediafire.com', na=False)].copy()\n",
    "\n",
    "# Function to check link status\n",
    "def check_mediafire_link(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124'}\n",
    "        response = requests.get(url, timeout=10, allow_redirects=True, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            if soup.find('a', {'id': 'downloadButton'}) or soup.find('div', {'class': 'filename'}):\n",
    "                return 'Alive'\n",
    "            return 'Dead'\n",
    "        return 'Dead'\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"URL: {url}\\nError: {str(e)}\\n\")\n",
    "        return 'Dead'\n",
    "\n",
    "# Check links concurrently with progress bar\n",
    "tqdm.pandas()\n",
    "def check_links_concurrently(links):\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        return list(tqdm(executor.map(check_mediafire_link, links), total=len(links)))\n",
    "\n",
    "mediafire_links['Status'] = check_links_concurrently(mediafire_links['Link'])\n",
    "\n",
    "# Save result to new Excel file\n",
    "output_file = r'C:\\Users\\PCS\\Downloads\\checked_mediafire_links.xlsx'\n",
    "if os.path.exists(output_file):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_file = r'C:\\Users\\PCS\\Downloads\\checked_mediafire_links_{timestamp}.xlsx'\n",
    "\n",
    "mediafire_links.to_excel(output_file, index=False)\n",
    "print(f\"Checking complete. Results saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows saved to: C:\\Users\\PCS\\Downloads\\nimra ahmed.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Input JSON file path\n",
    "json_file_path = r\"C:\\Users\\PCS\\Downloads\\neww.json\"\n",
    "\n",
    "# Input search name\n",
    "search_name = \"nimra ahmed\"  # Case-insensitive search term\n",
    "\n",
    "try:\n",
    "    # Load JSON data\n",
    "    df = pd.read_json(json_file_path)\n",
    "\n",
    "    # Filter rows where any column contains the search_name (case-insensitive)\n",
    "    filtered_df = df[df.apply(lambda row: row.astype(str).str.lower().str.contains(search_name.lower(), na=False).any(), axis=1)]\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(f\"No rows found containing '{search_name}'.\")\n",
    "    else:\n",
    "        # Output Excel file path (using search name)\n",
    "        excel_output_path = fr\"C:\\Users\\PCS\\Downloads\\{search_name}.xlsx\"\n",
    "\n",
    "        # Check if output file already exists\n",
    "        if os.path.exists(excel_output_path):\n",
    "            print(f\"Warning: Overwriting existing file at {excel_output_path}\")\n",
    "\n",
    "        # Save filtered rows to Excel\n",
    "        filtered_df.to_excel(excel_output_path, index=False, engine='openpyxl')\n",
    "        print(f\"Filtered rows saved to: {excel_output_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {json_file_path} was not found.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: Invalid JSON format in {json_file_path}. Details: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique novels saved to: C:\\Users\\PCS\\Downloads\\unique_novels_by_noor_rajpoot.xlsx\n",
      "Total unique novels found: 25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to clean and normalize titles\n",
    "def clean_title(title):\n",
    "    # Convert to lowercase\n",
    "    title = title.lower()\n",
    "    # Remove extra words like \"complete\", \"pdf\", \"urdu novel\", etc.\n",
    "    title = re.sub(r'\\b(complete|pdf|urdu novel|novel pdf|download|episode \\d+|\\d+ to \\d+|\\d+-\\d+|\\+\\d+\\+)\\b', '', title, flags=re.IGNORECASE)\n",
    "    # Remove \"by noor rajpoot\" since it's common in all titles\n",
    "    title = title.replace(\"by noor rajpoot\", \"\")\n",
    "    # Remove extra spaces and strip\n",
    "    title = ' '.join(title.split())\n",
    "    return title\n",
    "\n",
    "# Input Excel file path\n",
    "excel_input_path = r\"C:\\Users\\PCS\\Downloads\\noor rajpoot.xlsx\"\n",
    "\n",
    "try:\n",
    "    # Load the Excel file into a DataFrame\n",
    "    df = pd.read_excel(excel_input_path)\n",
    "\n",
    "    # Ensure the DataFrame has 'Title' and 'Link' columns\n",
    "    if 'Title' not in df.columns or 'Link' not in df.columns:\n",
    "        print(\"Error: Excel file must contain 'Title' and 'Link' columns.\")\n",
    "    else:\n",
    "        # Apply the cleaning function to the 'Title' column\n",
    "        df['Normalized_Title'] = df['Title'].apply(clean_title)\n",
    "\n",
    "        # Group by the normalized title to find unique novels\n",
    "        # Keep the first occurrence of each unique novel\n",
    "        unique_novels = df.groupby('Normalized_Title').first().reset_index()\n",
    "\n",
    "        # Drop the 'Normalized_Title' column if you don't want it in the output\n",
    "        unique_novels = unique_novels.drop(columns=['Normalized_Title'])\n",
    "\n",
    "        # Define the output path for the unique novels\n",
    "        excel_output_path = r\"C:\\Users\\PCS\\Downloads\\unique_novels_by_noor_rajpoot.xlsx\"\n",
    "\n",
    "        # Save the unique novels to a new Excel file\n",
    "        unique_novels.to_excel(excel_output_path, index=False, engine='openpyxl')\n",
    "\n",
    "        print(f\"Unique novels saved to: {excel_output_path}\")\n",
    "        print(f\"Total unique novels found: {len(unique_novels)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {excel_input_path} was not found.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: Invalid data in {excel_input_path}. Details: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Unique rows saved to: C:\\Users\\PCS\\Downloads\\filtered_unique_rows.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Load the Excel file\n",
    "excel_input_path = r\"C:\\Users\\PCS\\Downloads\\noor rajpoot.xlsx\" # 🔁 Replace with your file path\n",
    "df = pd.read_excel(excel_input_path)\n",
    "\n",
    "# Combine all columns into one string per row (for fuzzy comparison)\n",
    "df['combined'] = df.astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# List to store unique rows\n",
    "unique_rows = []\n",
    "used_indexes = set()\n",
    "\n",
    "# Loop to compare each row with all others\n",
    "for i in range(len(df)):\n",
    "    if i in used_indexes:\n",
    "        continue\n",
    "    current_row = df.iloc[i]\n",
    "    current_combined = current_row['combined']\n",
    "    unique_rows.append(current_row)\n",
    "    \n",
    "    for j in range(i + 1, len(df)):\n",
    "        if j in used_indexes:\n",
    "            continue\n",
    "        compare_combined = df.iloc[j]['combined']\n",
    "        similarity = fuzz.ratio(current_combined, compare_combined)\n",
    "        \n",
    "        if similarity >= 90:  # 🔁 Change threshold if needed\n",
    "            used_indexes.add(j)\n",
    "\n",
    "# Create new DataFrame from unique rows\n",
    "unique_df = pd.DataFrame(unique_rows).drop(columns=['combined'])\n",
    "\n",
    "# Save the result to a new Excel file\n",
    "output_path = r\"C:\\Users\\PCS\\Downloads\\filtered_unique_rows.xlsx\"\n",
    "unique_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"✅ Unique rows saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Unique rows (based on title similarity) saved to: C:\\Users\\PCS\\Downloads\\filtered_unique_rows.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Load the Excel file\n",
    "excel_input_path = r\"C:\\Users\\PCS\\Downloads\\faiza iftikhar.xlsx\" # Replace with your file path\n",
    "df = pd.read_excel(excel_input_path)\n",
    "\n",
    "# Ensure 'Titles' column exists\n",
    "if 'Titles' not in df.columns:\n",
    "    raise ValueError(\"Column 'Titles' not found in the Excel file\")\n",
    "\n",
    "# List to store unique rows\n",
    "unique_rows = []\n",
    "used_indexes = set()\n",
    "\n",
    "# Loop to compare titles of each row with all others\n",
    "for i in range(len(df)):\n",
    "    if i in used_indexes:\n",
    "        continue\n",
    "    current_row = df.iloc[i]\n",
    "    current_title = str(current_row['Titles'])  # Convert title to string\n",
    "    unique_rows.append(current_row)\n",
    "    \n",
    "    for j in range(i + 1, len(df)):\n",
    "        if j in used_indexes:\n",
    "            continue\n",
    "        compare_title = str(df.iloc[j]['Titles'])  # Convert title to string\n",
    "        similarity = fuzz.ratio(current_title, compare_title)\n",
    "        \n",
    "        if similarity >= 70:  # Change threshold if needed\n",
    "            used_indexes.add(j)\n",
    "\n",
    "# Create new DataFrame from unique rows\n",
    "unique_df = pd.DataFrame(unique_rows)\n",
    "\n",
    "# Save the result to a new Excel file\n",
    "output_path = r\"C:\\Users\\PCS\\Downloads\\filtered_unique_rows.xlsx\"\n",
    "unique_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"✅ Unique rows (based on title similarity) saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HTML file generated at: C:\\Users\\PCS\\Downloads\\novel_list.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the filtered Excel file\n",
    "excel_input_path = r\"C:\\Users\\PCS\\Downloads\\filtered_unique_rows.xlsx\"  # Replace with your file path\n",
    "df = pd.read_excel(excel_input_path)\n",
    "\n",
    "# Ensure required columns exist\n",
    "if 'title' not in df.columns or 'link' not in df.columns:\n",
    "    raise ValueError(\"Excel file must contain 'title' and 'link' columns\")\n",
    "\n",
    "# Start HTML content\n",
    "html_content = \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Novel List</title>\n",
    "    <style>\n",
    "        body { font-family: sans-serif; background-color: #f5f5f5; color: #202122; }\n",
    "        .novel-item { margin-bottom: 10px; }\n",
    "        .bullet { color: #1a1a1a; font-family: helvetica; font-size: 13px; }\n",
    "        .title { font-style: italic; font-size: 14px; margin-left: 20px; }\n",
    "        .link { font-style: italic; font-size: 14px; margin-left: 40px; }\n",
    "        a { color: #0645AD; text-decoration: none; }\n",
    "        a:hover { text-decoration: underline; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Novel List</h1>\n",
    "\"\"\"\n",
    "\n",
    "# Add each title and link in the specified format\n",
    "for index, row in df.iterrows():\n",
    "    title = str(row['title']).strip()\n",
    "    link = str(row['link']).strip()\n",
    "    html_content += f\"\"\"\n",
    "    <div class=\"novel-item\">\n",
    "        <p><i class=\"bullet\">⦿</i></p>\n",
    "        <div><i class=\"title\">{title}</i></div>\n",
    "        <div><i class=\"link\"><a href=\"{link}\" rel=\"nofollow\" target=\"_blank\">Download</a></i></div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "# Close HTML content\n",
    "html_content += \"\"\"\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Save the HTML content to a file\n",
    "output_html_path = r\"C:\\Users\\PCS\\Downloads\\novel_list.html\"\n",
    "with open(output_html_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(f\"✅ HTML file generated at: {output_html_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HTML file generated at: C:\\Users\\PCS\\Downloads\\novel_list.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the filtered Excel file\n",
    "excel_input_path = r\"C:\\Users\\PCS\\Downloads\\filtered_unique_rows.xlsx\"  # Replace with your file path\n",
    "df = pd.read_excel(excel_input_path)\n",
    "\n",
    "# Ensure required columns exist\n",
    "if 'title' not in df.columns or 'link' not in df.columns:\n",
    "    raise ValueError(\"Excel file must contain 'title' and 'link' columns\")\n",
    "\n",
    "# Start HTML content\n",
    "html_content = \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Novel List</title>\n",
    "    <style>\n",
    "        body { font-family: sans-serif; background-color: #f5f5f5; color: #202122; }\n",
    "        .novel-item { margin-bottom: 10px; }\n",
    "        .bullet { color: #1a1a1a; font-family: helvetica; font-size: 13px; }\n",
    "        .title { font-style: italic; font-size: 14px; margin-left: 20px; }\n",
    "        .link { font-style: italic; font-size: 14px; margin-left: 40px; }\n",
    "        a { color: #0645AD; text-decoration: none; }\n",
    "        a:hover { text-decoration: underline; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Novel List</h1>\n",
    "\"\"\"\n",
    "\n",
    "# Add each title and link in the specified format\n",
    "for index, row in df.iterrows():\n",
    "    title = str(row['title']).strip()\n",
    "    link = str(row['link']).strip()\n",
    "    html_content += f\"\"\"\n",
    "    <div class=\"novel-item\">\n",
    "        <p><i class=\"bullet\">⦿</i></p>\n",
    "        <div><i class=\"title\">{title}</i></div>\n",
    "        <div><i class=\"link\"><a href=\"{link}\" rel=\"nofollow\" target=\"_blank\">Download</a></i></div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "# Close HTML content\n",
    "html_content += \"\"\"\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Save the HTML content to a file\n",
    "output_html_path = r\"C:\\Users\\PCS\\Downloads\\novel_list.html\"\n",
    "with open(output_html_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(f\"✅ HTML file generated at: {output_html_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Excel file: ['title', 'link']\n",
      "✅ HTML content saved to: C:\\Users\\PCS\\Downloads\\novel_list.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the filtered Excel file\n",
    "excel_input_path = r\"C:\\Users\\PCS\\Downloads\\filtered_unique_rowssorted.xlsx\"# Replace with your file path\n",
    "try:\n",
    "    df = pd.read_excel(excel_input_path)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Excel file not found at: {excel_input_path}\")\n",
    "\n",
    "# Print columns for debugging\n",
    "print(\"Columns in Excel file:\", df.columns.tolist())\n",
    "\n",
    "# Rename columns to 'title' and 'link' (update 'Name' and 'URL' to match your Excel file)\n",
    "df = df.rename(columns={'Name': 'title', 'URL': 'link'})\n",
    "\n",
    "# Ensure required columns exist\n",
    "if 'title' not in df.columns or 'link' not in df.columns:\n",
    "    raise ValueError(f\"Excel file must contain 'title' and 'link' columns. Found: {df.columns.tolist()}\")\n",
    "\n",
    "# Sort DataFrame by title for alphabetical order\n",
    "df = df.sort_values(by='title', key=lambda x: x.str.lower())\n",
    "\n",
    "# Start HTML content (minimal wrapper, as your example is a fragment)\n",
    "html_content = \"\"\n",
    "\n",
    "# Add each title and link in the exact provided format with numbering\n",
    "for index, row in df.iterrows():\n",
    "    title = str(row['title']).strip() if pd.notna(row['title']) else \"Untitled\"\n",
    "    link = str(row['link']).strip() if pd.notna(row['link']) else \"#\"\n",
    "    # Handle special note for \"Maala\"\n",
    "    note = \" ( still running in episodes )\" if title.lower() == \"maala\" else \"\"\n",
    "    \n",
    "    # Mimic exact HTML structure from your example\n",
    "    html_content += f\"\"\"<p><i style=\"color: #1a1a1a; font-family: helvetica; font-size: 13px;\"><b>{index + 1}.</b></i></p>\n",
    "<div><span face=\"sans-serif\" style=\"color: #202122;\"><span style=\"font-size: 14px;\"><i style=\"color: #1a1a1a; font-family: helvetica; font-size: 13px;\">&nbsp;&nbsp;&nbsp;&nbsp;{title}</i></span></span></div>\n",
    "<div><span face=\"sans-serif\" style=\"color: #202122;\"><span style=\"font-size: 14px;\"><i style=\"color: #1a1a1a; font-family: helvetica; font-size: 13px;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"{link}\" rel=\"nofollow\" target=\"_blank\">👉Download</a>{note}</i></span></span></div>\n",
    "\"\"\"\n",
    "\n",
    "# Save the HTML content to a .txt file\n",
    "output_txt_path = r\"C:\\Users\\PCS\\Downloads\\novel_list.txt\"\n",
    "try:\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    print(f\"✅ HTML content saved to: {output_txt_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing TXT file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Excel file saved as C:\\Users\\PCS\\Downloads\\filtered_unique_rowssorted.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "input_file = r\"C:\\Users\\PCS\\Downloads\\filtered_unique_rows.xlsx\" # Replace with your input Excel file name\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Sort the DataFrame by the 'Titles' column\n",
    "df_sorted = df.sort_values(by='Titles', ascending=True)\n",
    "\n",
    "# Save the sorted DataFrame to a new Excel file\n",
    "output_file = r\"C:\\Users\\PCS\\Downloads\\filtered_unique_rowssorted.xlsx\"# Name of the new sorted Excel file\n",
    "df_sorted.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Sorted Excel file saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rapidfuzz import fuzz\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "# Load the Excel file\n",
    "excel_input_path = r\"D:\\workstation\\authors.xlsx\"  # Replace with your file path\n",
    "df = pd.read_excel(excel_input_path)\n",
    "\n",
    "# Ensure 'Author' column exists\n",
    "if 'Author' not in df.columns:\n",
    "    raise ValueError(\"Column 'Author' not found in the Excel file\")\n",
    "\n",
    "# Convert author column to string\n",
    "df['Author'] = df['Author'].astype(str)\n",
    "\n",
    "def compare_authors(args):\n",
    "    i, current_author, df_authors, start_idx = args\n",
    "    matches = []\n",
    "    for j in range(start_idx, len(df_authors)):\n",
    "        similarity = fuzz.ratio(current_author, df_authors[j])\n",
    "        if similarity >= 70:  # Adjust threshold as needed\n",
    "            matches.append(j)\n",
    "    return matches\n",
    "\n",
    "# Prepare inputs for parallel processing\n",
    "inputs = [(i, df['Author'].iloc[i], df['Author'], i + 1) for i in range(len(df))]\n",
    "\n",
    "# Use multiprocessing to parallelize comparisons\n",
    "with Pool() as pool:\n",
    "    results = pool.map(compare_authors, inputs)\n",
    "\n",
    "# Collect used indices\n",
    "used_indexes = set()\n",
    "for matches in results:\n",
    "    used_indexes.update(matches)\n",
    "\n",
    "# Create unique DataFrame\n",
    "unique_df = df[~df.index.isin(used_indexes)]\n",
    "\n",
    "# Generate output file path\n",
    "input_dir = os.path.dirname(excel_input_path)\n",
    "input_filename = os.path.splitext(os.path.basename(excel_input_path))[0]\n",
    "output_filename = f\"{input_filename}_unique_by_author.xlsx\"\n",
    "output_path = os.path.join(input_dir, output_filename)\n",
    "\n",
    "# Save the result\n",
    "unique_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"✅ Unique rows (based on author similarity) saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rapidfuzz import fuzz\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Load the Excel file\n",
    "excel_input_path = r\"D:\\workstation\\writers\\Noor Rajpoot.xlsx\"\n",
    "df = pd.read_excel(excel_input_path)\n",
    "\n",
    "# Ensure 'Author' column exists\n",
    "if 'Author' not in df.columns:\n",
    "    raise ValueError(\"Column 'Author' not found in the Excel file\")\n",
    "\n",
    "# Normalize and create blocking key (e.g., first 3 characters of author name)\n",
    "df['Author'] = df['Author'].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)\n",
    "df['block_key'] = df['Author'].str[:3]  # Block by first 3 characters\n",
    "\n",
    "# Function to process each block\n",
    "def process_block(block):\n",
    "    block_indices = block.index.tolist()\n",
    "    used_indices = set()\n",
    "    unique_indices = []\n",
    "    \n",
    "    for i, idx in enumerate(block_indices):\n",
    "        if idx in used_indices:\n",
    "            continue\n",
    "        unique_indices.append(idx)\n",
    "        current_author = block['Author'].iloc[i]\n",
    "        \n",
    "        for j in range(i + 1, len(block)):\n",
    "            if block_indices[j] in used_indices:\n",
    "                continue\n",
    "            compare_author = block['Author'].iloc[j]\n",
    "            if fuzz.ratio(current_author, compare_author) >= 70:\n",
    "                used_indices.add(block_indices[j])\n",
    "    \n",
    "    return unique_indices\n",
    "\n",
    "# Group by block_key and process in parallel\n",
    "blocks = [group for _, group in df.groupby('block_key')]\n",
    "with Pool() as pool:\n",
    "    results = pool.map(process_block, blocks)\n",
    "\n",
    "# Flatten unique indices\n",
    "unique_indices = [idx for sublist in results for idx in sublist]\n",
    "\n",
    "# Create unique DataFrame\n",
    "unique_df = df.iloc[unique_indices].drop(columns=['block_key'])\n",
    "\n",
    "# Generate output file path\n",
    "input_dir = os.path.dirname(excel_input_path)\n",
    "input_filename = os.path.splitext(os.path.basename(excel_input_path))[0]\n",
    "output_filename = f\"{input_filename}_unique_by_author.xlsx\"\n",
    "output_path = os.path.join(input_dir, output_filename)\n",
    "\n",
    "# Save the result\n",
    "unique_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"✅ Unique rows (based on author similarity) saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: D:\\workstation\\writers\\Noor Rajpoot.xlsx\n",
      "Normalizing author names and creating block keys...\n",
      "Processing blocks in parallel...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rapidfuzz import fuzz\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Load the Excel file\n",
    "    excel_input_path = r\"D:\\workstation\\writers\\Noor Rajpoot.xlsx\"\n",
    "    print(f\"Loading file: {excel_input_path}\")\n",
    "    df = pd.read_excel(excel_input_path)\n",
    "\n",
    "    # Ensure 'Author' column exists\n",
    "    if 'Author' not in df.columns:\n",
    "        raise ValueError(\"Column 'Author' not found in the Excel file\")\n",
    "\n",
    "    # Normalize and create blocking key (e.g., first 3 characters of author name)\n",
    "    print(\"Normalizing author names and creating block keys...\")\n",
    "    df['Author'] = df['Author'].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)\n",
    "    df['block_key'] = df['Author'].str[:3]  # Block by first 3 characters\n",
    "\n",
    "    # Function to process each block\n",
    "    def process_block(block):\n",
    "        block_indices = block.index.tolist()\n",
    "        used_indices = set()\n",
    "        unique_indices = []\n",
    "        \n",
    "        for i, idx in enumerate(block_indices):\n",
    "            if idx in used_indices:\n",
    "                continue\n",
    "            unique_indices.append(idx)\n",
    "            current_author = block['Author'].iloc[i]\n",
    "            \n",
    "            for j in range(i + 1, len(block)):\n",
    "                if block_indices[j] in used_indices:\n",
    "                    continue\n",
    "                compare_author = block['Author'].iloc[j]\n",
    "                if fuzz.ratio(current_author, compare_author) >= 70:\n",
    "                    used_indices.add(block_indices[j])\n",
    "        \n",
    "        return unique_indices\n",
    "\n",
    "    # Group by block_key and process in parallel\n",
    "    print(\"Processing blocks in parallel...\")\n",
    "    blocks = [group for _, group in df.groupby('block_key')]\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(process_block, blocks)\n",
    "\n",
    "    # Flatten unique indices\n",
    "    print(\"Flattening unique indices...\")\n",
    "    unique_indices = [idx for sublist in results for idx in sublist]\n",
    "\n",
    "    # Create unique DataFrame\n",
    "    print(\"Creating unique DataFrame...\")\n",
    "    unique_df = df.iloc[unique_indices].drop(columns=['block_key'])\n",
    "\n",
    "    # Generate output file path\n",
    "    input_dir = os.path.dirname(excel_input_path)\n",
    "    input_filename = os.path.splitext(os.path.basename(excel_input_path))[0]\n",
    "    output_filename = f\"{input_filename}_unique_by_author.xlsx\"\n",
    "    output_path = os.path.join(input_dir, output_filename)\n",
    "\n",
    "    # Save the result\n",
    "    print(f\"Saving output to: {output_path}\")\n",
    "    unique_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "\n",
    "    print(f\"✅ Unique rows (based on author similarity) saved to: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred: {str(e)}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Calculate and display elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"⏱️ Total execution time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: D:\\workstation\\writers\\Noor Rajpoot.xlsx\n",
      "Loaded 39 rows from the Excel file\n",
      "Normalizing author names and creating block keys...\n",
      "Created 1 unique block keys\n",
      "Processing blocks in parallel...\n",
      "Total blocks to process: 1\n",
      "❌ An error occurred: Pool.map() got an unexpected keyword argument 'timeout'\n",
      "Full traceback:\n",
      "⏱️ Total execution time: 0.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PCS\\AppData\\Local\\Temp\\ipykernel_13408\\1369605252.py\", line 57, in <module>\n",
      "    results = pool.map(process_block, blocks, timeout=300)\n",
      "TypeError: Pool.map() got an unexpected keyword argument 'timeout'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Pool is still running",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 92\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Ensure pool is closed properly\u001b[39;00m\n\u001b[32m     91\u001b[39m mp.Pool().close()\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[43mmp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\pool.py:662\u001b[39m, in \u001b[36mPool.join\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    660\u001b[39m util.debug(\u001b[33m'\u001b[39m\u001b[33mjoining pool\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == RUN:\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPool is still running\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (CLOSE, TERMINATE):\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIn unknown state\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Pool is still running"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rapidfuzz import fuzz\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import traceback\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Load the Excel file\n",
    "    excel_input_path = r\"D:\\workstation\\writers\\Noor Rajpoot.xlsx\"\n",
    "    print(f\"Loading file: {excel_input_path}\")\n",
    "    df = pd.read_excel(excel_input_path)\n",
    "    print(f\"Loaded {len(df)} rows from the Excel file\")\n",
    "\n",
    "    # Ensure 'Author' column exists\n",
    "    if 'Author' not in df.columns:\n",
    "        raise ValueError(\"Column 'Author' not found in the Excel file\")\n",
    "\n",
    "    # Normalize and create blocking key (e.g., first 3 characters of author name)\n",
    "    print(\"Normalizing author names and creating block keys...\")\n",
    "    df['Author'] = df['Author'].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)\n",
    "    df['block_key'] = df['Author'].str[:3]  # Block by first 3 characters\n",
    "    print(f\"Created {len(df['block_key'].unique())} unique block keys\")\n",
    "\n",
    "    # Function to process each block\n",
    "    def process_block(block):\n",
    "        block_indices = block.index.tolist()\n",
    "        used_indices = set()\n",
    "        unique_indices = []\n",
    "        \n",
    "        for i, idx in enumerate(block_indices):\n",
    "            if idx in used_indices:\n",
    "                continue\n",
    "            unique_indices.append(idx)\n",
    "            current_author = block['Author'].iloc[i]\n",
    "            \n",
    "            for j in range(i + 1, len(block)):\n",
    "                if block_indices[j] in used_indices:\n",
    "                    continue\n",
    "                compare_author = block['Author'].iloc[j]\n",
    "                if fuzz.ratio(current_author, compare_author) >= 70:\n",
    "                    used_indices.add(block_indices[j])\n",
    "        \n",
    "        return unique_indices\n",
    "\n",
    "    # Group by block_key and process in parallel with timeout\n",
    "    print(\"Processing blocks in parallel...\")\n",
    "    blocks = [group for _, group in df.groupby('block_key')]\n",
    "    print(f\"Total blocks to process: {len(blocks)}\")\n",
    "    \n",
    "    # Set a timeout for multiprocessing (e.g., 300 seconds = 5 minutes)\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(process_block, blocks, timeout=300)\n",
    "    \n",
    "    # Flatten unique indices\n",
    "    print(\"Flattening unique indices...\")\n",
    "    unique_indices = [idx for sublist in results for idx in sublist]\n",
    "    print(f\"Found {len(unique_indices)} unique rows\")\n",
    "\n",
    "    # Create unique DataFrame\n",
    "    print(\"Creating unique DataFrame...\")\n",
    "    unique_df = df.iloc[unique_indices].drop(columns=['block_key'])\n",
    "\n",
    "    # Generate output file path\n",
    "    input_dir = os.path.dirname(excel_input_path)\n",
    "    input_filename = os.path.splitext(os.path.basename(excel_input_path))[0]\n",
    "    output_filename = f\"{input_filename}_unique_by_author.xlsx\"\n",
    "    output_path = os.path.join(input_dir, output_filename)\n",
    "\n",
    "    # Save the result\n",
    "    print(f\"Saving output to: {output_path}\")\n",
    "    unique_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "\n",
    "    print(f\"✅ Unique rows (based on author similarity) saved to: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred: {str(e)}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Calculate and display elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"⏱️ Total execution time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Ensure pool is closed properly\n",
    "mp.Pool().close()\n",
    "mp.Pool().join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: D:\\workstation\\writers\\Noor Rajpoot.xlsx\n",
      "Loaded 39 rows from the Excel file\n",
      "Normalizing author names and creating block keys...\n",
      "Created 1 unique block keys\n",
      "Processing blocks in parallel...\n",
      "Total blocks to process: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rapidfuzz import fuzz\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import traceback           #fail\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize pool variable to ensure proper closure\n",
    "pool = None\n",
    "\n",
    "try:\n",
    "    # Load the Excel file\n",
    "    excel_input_path = r\"D:\\workstation\\writers\\Noor Rajpoot.xlsx\"\n",
    "    print(f\"Loading file: {excel_input_path}\")\n",
    "    df = pd.read_excel(excel_input_path)\n",
    "    print(f\"Loaded {len(df)} rows from the Excel file\")\n",
    "\n",
    "    # Ensure 'Author' column exists\n",
    "    if 'Author' not in df.columns:\n",
    "        raise ValueError(\"Column 'Author' not found in the Excel file\")\n",
    "\n",
    "    # Normalize and create blocking key (e.g., first 3 characters of author name)\n",
    "    print(\"Normalizing author names and creating block keys...\")\n",
    "    df['Author'] = df['Author'].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)\n",
    "    df['block_key'] = df['Author'].str[:3]  # Block by first 3 characters\n",
    "    print(f\"Created {len(df['block_key'].unique())} unique block keys\")\n",
    "\n",
    "    # Function to process each block\n",
    "    def process_block(block):\n",
    "        block_indices = block.index.tolist()\n",
    "        used_indices = set()\n",
    "        unique_indices = []\n",
    "        \n",
    "        for i, idx in enumerate(block_indices):\n",
    "            if idx in used_indices:\n",
    "                continue\n",
    "            unique_indices.append(idx)\n",
    "            current_author = block['Author'].iloc[i]\n",
    "            \n",
    "            for j in range(i + 1, len(block)):\n",
    "                if block_indices[j] in used_indices:\n",
    "                    continue\n",
    "                compare_author = block['Author'].iloc[j]\n",
    "                if fuzz.ratio(current_author, compare_author) >= 70:\n",
    "                    used_indices.add(block_indices[j])\n",
    "        \n",
    "        return unique_indices\n",
    "\n",
    "    # Group by block_key and process in parallel\n",
    "    print(\"Processing blocks in parallel...\")\n",
    "    blocks = [group for _, group in df.groupby('block_key')]\n",
    "    print(f\"Total blocks to process: {len(blocks)}\")\n",
    "    \n",
    "    # Initialize pool and process blocks\n",
    "    pool = Pool()\n",
    "    results = pool.map(process_block, blocks)\n",
    "    print(\"Finished processing blocks\")\n",
    "    \n",
    "    # Flatten unique indices\n",
    "    print(\"Flattening unique indices...\")\n",
    "    unique_indices = [idx for sublist in results for idx in sublist]\n",
    "    print(f\"Found {len(unique_indices)} unique rows\")\n",
    "\n",
    "    # Create unique DataFrame\n",
    "    print(\"Creating unique DataFrame...\")\n",
    "    unique_df = df.iloc[unique_indices].drop(columns=['block_key'])\n",
    "\n",
    "    # Generate output file path\n",
    "    input_dir = os.path.dirname(excel_input_path)\n",
    "    input_filename = os.path.splitext(os.path.basename(excel_input_path))[0]\n",
    "    output_filename = f\"{input_filename}_unique_by_author.xlsx\"\n",
    "    output_path = os.path.join(input_dir, output_filename)\n",
    "\n",
    "    # Save the result\n",
    "    print(f\"Saving output to: {output_path}\")\n",
    "    unique_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "\n",
    "    print(f\"✅ Unique rows (based on author similarity) saved to: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred: {str(e)}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Close the pool if it was created\n",
    "    if pool is not None:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    # Calculate and display elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"⏱️ Total execution time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: D:\\workstation\\writers\\Noor Rajpoot.xlsx\n",
      "Loaded 39 rows from the Excel file\n",
      "Normalizing author names and creating block keys...\n",
      "Created 1 unique block keys\n",
      "Block size stats: min=39, max=39, mean=39.00\n",
      "Grouping blocks...\n",
      "Total blocks to process: 1\n",
      "Processing blocks in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing blocks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rapidfuzz import fuzz\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import traceback               #FAil\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize pool variable\n",
    "pool = None\n",
    "\n",
    "try:\n",
    "    # Load the Excel file\n",
    "    excel_input_path = r\"D:\\workstation\\writers\\Noor Rajpoot.xlsx\"\n",
    "    print(f\"Loading file: {excel_input_path}\")\n",
    "    df = pd.read_excel(excel_input_path)\n",
    "    print(f\"Loaded {len(df)} rows from the Excel file\")\n",
    "\n",
    "    # Ensure 'Author' column exists\n",
    "    if 'Author' not in df.columns:\n",
    "        raise ValueError(\"Column 'Author' not found in the Excel file\")\n",
    "\n",
    "    # Normalize and create blocking key\n",
    "    print(\"Normalizing author names and creating block keys...\")\n",
    "    df['Author'] = df['Author'].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)\n",
    "    df['block_key'] = df['Author'].str[:3]  # Block by first 3 characters\n",
    "    print(f\"Created {len(df['block_key'].unique())} unique block keys\")\n",
    "\n",
    "    # Log block sizes to identify large blocks\n",
    "    block_sizes = df.groupby('block_key').size()\n",
    "    print(f\"Block size stats: min={block_sizes.min()}, max={block_sizes.max()}, mean={block_sizes.mean():.2f}\")\n",
    "    large_blocks = block_sizes[block_sizes > 100]\n",
    "    if not large_blocks.empty:\n",
    "        print(f\"Warning: {len(large_blocks)} blocks have more than 100 rows: {large_blocks.to_dict()}\")\n",
    "\n",
    "    # Function to process each block\n",
    "    def process_block(block):\n",
    "        block_indices = block.index.tolist()\n",
    "        used_indices = set()\n",
    "        unique_indices = []\n",
    "        \n",
    "        for i, idx in enumerate(block_indices):\n",
    "            if idx in used_indices:\n",
    "                continue\n",
    "            unique_indices.append(idx)\n",
    "            current_author = block['Author'].iloc[i]\n",
    "            \n",
    "            for j in range(i + 1, len(block)):\n",
    "                if block_indices[j] in used_indices:\n",
    "                    continue\n",
    "                compare_author = block['Author'].iloc[j]\n",
    "                if fuzz.ratio(current_author, compare_author) >= 70:\n",
    "                    used_indices.add(block_indices[j])\n",
    "        \n",
    "        return unique_indices\n",
    "\n",
    "    # Group by block_key\n",
    "    print(\"Grouping blocks...\")\n",
    "    blocks = [group for _, group in df.groupby('block_key')]\n",
    "    print(f\"Total blocks to process: {len(blocks)}\")\n",
    "\n",
    "    # Try parallel processing\n",
    "    try:\n",
    "        print(\"Processing blocks in parallel...\")\n",
    "        pool = Pool()\n",
    "        results = []\n",
    "        # Use tqdm to show progress\n",
    "        for result in tqdm(pool.imap(process_block, blocks), total=len(blocks), desc=\"Processing blocks\"):\n",
    "            results.append(result)\n",
    "        print(\"Finished processing blocks\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Parallel processing failed: {str(e)}\")\n",
    "        print(\"Falling back to single-threaded processing...\")\n",
    "        results = [process_block(block) for block in tqdm(blocks, desc=\"Processing blocks (single-threaded)\")]\n",
    "    \n",
    "    # Flatten unique indices\n",
    "    print(\"Flattening unique indices...\")\n",
    "    unique_indices = [idx for sublist in results for idx in sublist]\n",
    "    print(f\"Found {len(unique_indices)} unique rows\")\n",
    "\n",
    "    # Create unique DataFrame\n",
    "    print(\"Creating unique DataFrame...\")\n",
    "    unique_df = df.iloc[unique_indices].drop(columns=['block_key'])\n",
    "\n",
    "    # Generate output file path\n",
    "    input_dir = os.path.dirname(excel_input_path)\n",
    "    input_filename = os.path.splitext(os.path.basename(excel_input_path))[0]\n",
    "    output_filename = f\"{input_filename}_unique_by_author.xlsx\"\n",
    "    output_path = os.path.join(input_dir, output_filename)\n",
    "\n",
    "    # Save the result\n",
    "    print(f\"Saving output to: {output_path}\")\n",
    "    unique_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "\n",
    "    print(f\"✅ Unique rows (based on author similarity) saved to: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred: {str(e)}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Close the pool if it was created\n",
    "    if pool is not None:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    # Calculate and display elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"⏱️ Total execution time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: D:\\workstation\\writers\\Noor Rajpoot.xlsx\n",
      "Loaded 39 rows from the Excel file\n",
      "Normalizing author names and creating block keys...\n",
      "Created 1 unique block keys\n",
      "Block size stats: min=39, max=39, mean=39.00\n",
      "Grouping blocks...\n",
      "Total blocks to process: 1\n",
      "Processing blocks in parallel...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rapidfuzz import fuzz, process\n",
    "from pebble import ProcessPool           #FAIL\n",
    "import time\n",
    "import traceback\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize pool variable\n",
    "pool = None\n",
    "\n",
    "try:\n",
    "    # Load the Excel file\n",
    "    excel_input_path = r\"D:\\workstation\\writers\\Noor Rajpoot.xlsx\"\n",
    "    print(f\"Loading file: {excel_input_path}\")\n",
    "    df = pd.read_excel(excel_input_path)\n",
    "    print(f\"Loaded {len(df)} rows from the Excel file\")\n",
    "\n",
    "    # Ensure 'Author' column exists\n",
    "    if 'Author' not in df.columns:\n",
    "        raise ValueError(\"Column 'Author' not found in the Excel file\")\n",
    "\n",
    "    # Normalize and create blocking key (use first 4 characters for smaller blocks)\n",
    "    print(\"Normalizing author names and creating block keys...\")\n",
    "    df['Author'] = df['Author'].astype(str).str.lower().str.replace(r'\\s+', '', regex=True)\n",
    "    df['block_key'] = df['Author'].str[:4]  # Increased to 4 characters\n",
    "    print(f\"Created {len(df['block_key'].unique())} unique block keys\")\n",
    "\n",
    "    # Log block sizes\n",
    "    block_sizes = df.groupby('block_key').size()\n",
    "    print(f\"Block size stats: min={block_sizes.min()}, max={block_sizes.max()}, mean={block_sizes.mean():.2f}\")\n",
    "    large_blocks = block_sizes[block_sizes > 100]\n",
    "    if not large_blocks.empty:\n",
    "        print(f\"Warning: {len(large_blocks)} blocks have more than 100 rows: {large_blocks.to_dict()}\")\n",
    "\n",
    "    # Function to process each block with optimized fuzzy matching\n",
    "    def process_block(block):\n",
    "        block_indices = block.index.tolist()\n",
    "        used_indices = set()\n",
    "        unique_indices = []\n",
    "        \n",
    "        if len(block_indices) <= 100:  # Use pairwise comparison for small blocks\n",
    "            for i, idx in enumerate(block_indices):\n",
    "                if idx in used_indices:\n",
    "                    continue\n",
    "                unique_indices.append(idx)\n",
    "                current_author = block['Author'].iloc[i]\n",
    "                \n",
    "                for j in range(i + 1, len(block)):\n",
    "                    if block_indices[j] in used_indices:\n",
    "                        continue\n",
    "                    compare_author = block['Author'].iloc[j]\n",
    "                    if fuzz.ratio(current_author, compare_author) >= 70:\n",
    "                        used_indices.add(block_indices[j])\n",
    "        else:  # Use rapidfuzz.process.extract for large blocks\n",
    "            authors = block['Author'].tolist()\n",
    "            for i, idx in enumerate(block_indices):\n",
    "                if idx in used_indices:\n",
    "                    continue\n",
    "                unique_indices.append(idx)\n",
    "                current_author = authors[i]\n",
    "                # Extract similar authors with score >= 70\n",
    "                matches = process.extract(current_author, authors[i+1:], scorer=fuzz.ratio, score_cutoff=70)\n",
    "                for match, score, match_idx in matches:\n",
    "                    actual_idx = block_indices[i + 1 + match_idx]\n",
    "                    if actual_idx not in used_indices:\n",
    "                        used_indices.add(actual_idx)\n",
    "        \n",
    "        return unique_indices\n",
    "\n",
    "    # Group by block_key\n",
    "    print(\"Grouping blocks...\")\n",
    "    blocks = [group for _, group in df.groupby('block_key')]\n",
    "    print(f\"Total blocks to process: {len(blocks)}\")\n",
    "\n",
    "    # Try parallel processing with timeout\n",
    "    try:\n",
    "        print(\"Processing blocks in parallel...\")\n",
    "        pool = ProcessPool(max_workers=4)  # Limit to 4 processes\n",
    "        results = []\n",
    "        # Use tqdm for progress and timeout of 60 seconds per block\n",
    "        for result in tqdm(pool.map(process_block, blocks, timeout=60).result(), total=len(blocks), desc=\"Processing blocks\"):\n",
    "            if isinstance(result, Exception):\n",
    "                print(f\"Warning: A block failed with error: {result}\")\n",
    "                continue\n",
    "            results.append(result)\n",
    "        print(\"Finished processing blocks\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Parallel processing failed: {str(e)}\")\n",
    "        print(\"Falling back to single-threaded processing...\")\n",
    "        results = [process_block(block) for block in tqdm(blocks, desc=\"Processing blocks (single-threaded)\")]\n",
    "    \n",
    "    # Flatten unique indices\n",
    "    print(\"Flattening unique indices...\")\n",
    "    unique_indices = [idx for sublist in results for idx in sublist]\n",
    "    print(f\"Found {len(unique_indices)} unique rows\")\n",
    "\n",
    "    # Create unique DataFrame\n",
    "    print(\"Creating unique DataFrame...\")\n",
    "    unique_df = df.iloc[unique_indices].drop(columns=['block_key'])\n",
    "\n",
    "    # Generate output file path\n",
    "    input_dir = os.path.dirname(excel_input_path)\n",
    "    input_filename = os.path.splitext(os.path.basename(excel_input_path))[0]\n",
    "    output_filename = f\"{input_filename}_unique_by_author.xlsx\"\n",
    "    output_path = os.path.join(input_dir, output_filename)\n",
    "\n",
    "    # Save the result\n",
    "    print(f\"Saving output to: {output_path}\")\n",
    "    unique_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "\n",
    "    print(f\"✅ Unique rows (based on author similarity) saved to: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred: {str(e)}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Close the pool if it was created\n",
    "    if pool is not None:\n",
    "        pool.close()  \n",
    "        pool.join()\n",
    "    # Calculate and display elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"⏱️ Total execution time: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
